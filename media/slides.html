<!DOCTYPE html>

<!--
  Google HTML5 slide template

  Authors: Luke MahÃ© (code)
           Marcin Wichary (code and design)
           
           Dominic Mazzoni (browser compatibility)
           Charles Chen (ChromeVox support)

  URL: http://code.google.com/p/html5slides/
-->


<!--

  These slides were created for the H@B workshop on 12/1.
  
  They brifly cover probability, bayes nets, naive bayes, and 
  information gain. 

  "Building" is OFF by default for easy viewing. Run "present()" in 
  a js console before presentations to turn it on.

  Yes, for those of you who disapprove I've used <center> tags 
  very liberally. I was lazy to write more CSS

-->


<html>
  <head>
    <title>H@B Artificial Intelligence Workshop</title>

    <meta charset='utf-8'>
    <script
      src='http://html5slides.googlecode.com/svn/trunk/slides.js'></script>

  </head>
  
  <link rel="stylesheet" href="index.css">

  <body style='display: none'>

    <section class='slides layout-regular'>
      
      <!-- Your slides (<article>s) go here. Delete or comment out the
           slides below. -->

      <article>
        <h1>
          Replace Your Brain
        </h1>
        <br />
        <h3>AI Workshop</h3>
        <p id="slide-credit" class="smaller">Slides originally by Allen Chen, Akihiro Matsukawa</p>
        <p>Achal Dave, Sharad Vikram, Peter Gao</p>
      </article>
      <article>
        <h1>
          What is Artificial Intelligence?
        </h1>
      </article>

      <article>
        <h3>What is Artificial Intelligence?</h3>
        <p>Not just about making human-like robots
          <span class="indent-more smaller" style="display: block">
            (But that is an awesome part of it)
          </span>
        </p>
        <br />
        <p>
          Basically, giving intelligence to computers
        </p>
        <ul>
          <li>
            Playing tic tac toe, chess, and of course, Pacman
          </li>
          <li>
            Self-driving cars
          </li>
          <li>
            Lots, lots more
          </li>
        </ul>
      </article>
        
      <article>
        <h1>
          Today: 
        </h1>
        <br />
        <h3>Sentiment Analysis using 
          a Naive Bayes Classifier
        </h3>
      </article>
      
      <article>
        <h3>
          Today...
        </h3>
        <ul>
          <li>
            What is probability, anyway?
          </li><br>
          <li>
            How do Bayesian networks work?
          </li><br>
          <li>
            What are some examples of Bayesian Networks? <br>
              <ul>
                <li> Naive Bayes Classifier</li>
                <li> Markov chains </li>
              </ul>
          </li><br>
          <li>
            Can you give me an example?
          </li>
        </ul>
      </article>

      <article>
        <h2>
          Probability
        </h2>
      </article>

      <article>
        <h3>
          Probability - Basics
        </h3>
        <p>
          We use probability to describe the <b>likelihood</b> of an event
        </p>

        <h4>Details</h4>
        <ul>
          <li> 
            The <span class="definition">sample space</span> is a fancy term
            for all the possible things (<span
            class="definition">outcomes</span>) that could happen. 
          </li>
          <li>
          A <span class="definition">probability function</span> tells you what
          the probability of a particular outcome happening are.
          </li>
        </ul>
      </article>

      <article>
        <h3>
          Probability - Condition, Independence
        </h3>
        <p>
          By example:
        </p>
        <p>
          Event A: Ms. Pacman gets an A in 61B
        </p>
        <p>
          Event B: Professor Hilfinger is teaching 61B
        </p>
        <p>
          <span class="definition">Conditional probability</span>: probability
          of event A, given B happened <img class="eqn"
          src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black}
          P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P\left(B\right)}}"
          title="\LARGE {\color{Black}
          P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P\left(B\right)}}" />
        </p>
        <p>
          <span class="definition">Independence</span>: When B doesn't
          influence the probability of A (and vice versa), A and B are
          independent.
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(A|B\right)=P\left(A\right) }" title="\LARGE {\color{Black} P\left(A|B\right)=P\left(A\right) }" />
        </p>
      </article>

      <article>
        <h2>
          Bayesian Networks, Naive Bayes
        </h2>
      </article>

      <article>
        <h3>
          Classifying
        </h3>
        <p>
          Why?
        </p>
        <ul>
          <li>Email!</li>
          <li>Winning at Pacman</li>
          <li><del>Building Skynet</del></li>
        </ul>
      </article>
      <article>
        <h3>
          Classifier
        </h3>
        <ul>
          <li>
            Define categories, or <span class="definition">classes</span> (eg. "spam", "important", "piazza spam")
          </li>
          <li>
            Pick observations, or <span class="definition">features</span>, that give us insight for the class.
          </li>
        </ul>
        <p>
          A <b>classifier</b> takes in features, guesses a class <br><br>
          <center>
          <img src="classifier.png" width=520>
          </center>
        </p>
      </article>

      <article>
        <h3>
          Bayesian Networks
        </h3>
        <ul>
          <li>
            Graphical representation of a <b>probability model</b>
          </li>
          <li>
            The circles, or <span class="definition">nodes</span>, represent variables
          </li>
          <li>
            The arrow notes that a variable "influences" another
          </li>
        </ul>
        <img class="center" width=450 src="http://upload.wikimedia.org/wikipedia/en/thumb/0/0e/SimpleBayesNet.svg/500px-SimpleBayesNet.svg.png" />
      </article>
      <article>
        <h3>
          Bayesian Networks
        </h3>
        <p>
          <img class="center" width=450 src="http://upload.wikimedia.org/wikipedia/en/thumb/0/0e/SimpleBayesNet.svg/500px-SimpleBayesNet.svg.png" />
          <ul>
            <li>
              The probability of any variable is the product of its probabilities
              conditioned on all its parents.
            </li>
            <li>
              Here, for example:
            </li>
              <img class="eqn center" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(+G, +S, +R \right) = P\left(+G | +S, +R \right) * P\left(+S | +R \right) * P\left(+R\right)}" title="\LARGE {\color{Black} P\left(G\right) = P\left(G | S\right) * P\left(G | R\right)}" />
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Definition
        </h3>
        <ul>
          <li>
            <span class="definition">Naive Bayes</span> classifier, using Bayesian networks
          </li>
          <li>
            One <i>root</i> node, which is the <i>class</i> (e.g. "spam or not")
          </li>
          <li>
          Root node points to many <i>child</i> nodes, the <i>features</i>
          </li>
          <li>
            A little advanced: What does "naive" mean?
          </li>
        </ul>
        <p>
          <br>
          <center>
          <img src="NB.png" width=600/>
          </center>
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Inference
        </h3>
        <p>
          So you have the values of features; how do you pick the class?
        </p>
        <p>
          Pick the class c with the highest probability given these features!
        </p>
        <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=c|F_{1},\ldots,F_{n}\right)}" title="\LARGE {\color{Black} P\left(C=c|F_{1},\ldots,F_{n}\right)}" />
      </article>
      <article>
        <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=c|F_{1},\ldots,F_{n}\right)}" title="\LARGE {\color{Black} P\left(C=c|F_{1},\ldots,F_{n}\right)}" />
        <p>
          How do I find this conditonal probability?
          <p class="indent-more">Some seemingly hard math</p>
        </p>
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" title="\LARGE {\color{Black} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" />
          <img class=eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C|F_{1},\ldots,F_{n}\right)\propto P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" title="\LARGE {\color{Black} P\left(C|F_{1},\ldots,F_{n}\right) \propto P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" />
        </p>
      </article>
      
      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will build a classifier with two classes and <b>binary features</b>. 
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" width="600" />
          </center>
        </p>
        <p>
          "The concert was great."
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" title="\LARGE {\color{Black} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" />-->
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" title="\LARGE {\color{Black} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" />-->
        </p>
        <div style="position:absolute; bottom:100px; right: 100px; padding: 5px; border:solid gray 1px">
          POSITIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will build a classifier with two classes and <b>binary features</b>. 
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" width="600"/>
          </center>
        </p>
        <p>
          "I hate doing the dishes."
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" title="\LARGE {\color{Black} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" />-->
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" title="\LARGE {\color{Black} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" />-->
        </p>
        <div style="position:absolute; bottom:100px; right: 100px; padding: 5px; border:solid gray 1px">
          NEGATIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will build a classifier with two classes and <b>binary features</b>. 
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" width="600"/>
          </center>
        </p>
        <p>
          "I ate cookies."
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" title="\LARGE {\color{Black} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" />-->
          <!--<img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" title="\LARGE {\color{Black} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" />-->
        </p>
        <div style="position:absolute; bottom:40px; right: 50px; padding: 5px; border:solid gray 1px">
          DON'T KNOW (not confident)
        <div>
      </article>

      <article>
        <h3>
          A few more things...
        </h3>
        <ul>
          <li>
            When we have a lot of features, multiplying probabilities gives you super small numbers.
          </li>
          <li>
            Instead, use sum of log probabilities. 
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" title="\LARGE {\color{Black} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" />
          </li>
        </ul>
      </article>
      <article>
        <h1> Let's build it! </h1>
        <br />
        <h3>Download: <a href="http://bit.ly/hab-ai">bit.ly/hab-ai</a></h3>
        <br />
        <br />
        <p class="smaller">Source: <a href="https://github.com/achalddave/twitter_sentiment">github.com/achalddave/twitter_sentiment</a></p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p> 
          As we saw in the example, "the" was not a very useful feature.
          <b>But how do we know which one is useful?</b>
        </p>
        <p>
          Answer: we will use <b>entropy</b> and <b>information gain</b>
        </p>
      </article>
      <article>
        <h3>Entropy</h3>
        <p>
          <b>Entropy</b> is a measure of unpredictability in a set.
        </p>
        <p class="indent-more">
          E.g. flipping a fair (high entropy) vs biased coin (lower entropy)
        </p>
        <p>
          <b>Information gain</b> tells us the <em>improvement</em> in the entropy of a set S by splitting it into k small subsets. 
        </p>
        <p class="indent-more">
          E.g. split into "positive" and "negative" categories
        </p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p>
          Information gain gives a measure of how "informative" a feature is.
        </p>
        <p>
          So: pick features with the <b>highest</b> information gain.
        </p>
      </article>
      <article>
        <h3>Advanced: Some math</h3>
        <p>
          The entropy of a set S is
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} H\left(S\right)= - \sum_{i}p_{i}\log\left(p_{i}\right) }" title="\LARGE {\color{Black} H\left(S\right)= - \sum_{i}p_{i}\log\left(p_{i}\right) }" />
        </p>
        <p>
          Meanwhile, the information gain by splitting S into k subsets is
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" title="\LARGE {\color{Black} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" />
        </p>
      </article>
      <article>
        <h3>Advanced: More math</h3>
        <p>
          The entropy of a boolean distribution (a Bernoulli) is
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Black} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" title="\LARGE {\color{Black} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" />
          Information gain of a particular word is 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?{\color{Black} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" title="{\color{Black} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" />
          <br>
          <i>
            If I split my tweets into two sets (+ve/-ve), which words am I most likely to see in one
            but not in the other?
          </i>
          </ul>
        </p>
      </article>

      <article>
        <h3> Where can I learn more? </h3>
        <ul>
          <li>
              Markov chains: <a href="http://en.wikipedia.org/wiki/Markov_chain">http://en.wikipedia.org/wiki/Markov_chain</a><br>
	  </li>
	  <li>
              Machine Learning: <a href="http://en.wikipedia.org/wiki/Machine_learning">http://en.wikipedia.org/wiki/Machine_learning</a><br>
	  </li>
	  <li>
              Toy Projects: <a href="https://www.quora.com/Machine-Learning/What-are-some-good-learning-projects-to-teach-oneself-about-machine-learning">https://www.quora.com/Machine-Learning/What-are-some-good-learning-projects-to-teach-oneself-about-machine-learning</a><br>
	  </li>
	  <li>
              CS188: <a href="https://berkeley.edx.org/">https://berkeley.edx.org/</a><br>
	  </li>
	  <li>
              OpenCV: <a href="http://opencv.willowgarage.com/wiki/">http://opencv.willowgarage.com/wiki/</a><br>
          </li>
        </ul>
      </article>

    </section>

  </body>
</html>
